{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will find the protocal where the observed sequences in the testing dataset is created. In the folder `training_data/degradataion_data`, you will find the $50$ complete run-to-failure sequences. These sequences are generated from a theoretical model which contains three failure modes:\n",
    "- infant mortality\n",
    "- control board failure\n",
    "- crack growth failure\n",
    "The three failure modes are \"competing\", i.e., whichever failure mode occurs first will be the one that causes the failure. It is \"run-to-failure\" as the crack growth process is monitored until a failure occurs (but the failure could be due to either one of the three failure modes).\n",
    "\n",
    "The testing dataset you have in the folder `testing_data/group_0` is created by based on the complete run-to-failure sequences by randomly truncated them at a given time slot $t_end,$ and you need to predict the remaining useful life from this time point $t_end.$ The truncation is done following the same manner:\n",
    "- If the run-to-failure sequence is shorter than $7$, i.e., if the time-to-failure is less than or equal to $6$, we keep the sequence as it is.\n",
    "- If the run-to-failure sequence is longer than $7$, i.e., if the time-to-failure is greater than $6$, it is truncated at a random time point $t_end$, which is generated from a uniform distribution from [1, ttf-1].\n",
    "\n",
    "The following code will generate a \"pseudo\" testing dataset based on the training dataset, which can be used by you to evaluate the performance of the model you developed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "group = 0\n",
    "\n",
    "directory = 'pseudo_testing_data_with_truth'\n",
    "directory_truth = 'degradation_data'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory_truth) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate over the shuffled file list and rename the files\n",
    "for i, file_name in enumerate(csv_files):\n",
    "    df = pd.read_csv(directory_truth + '/' + file_name)\n",
    "    ttf = df.iloc[0]['rul (months)']\n",
    "    if ttf >= 6:\n",
    "        random_integer = random.randint(1, ttf-1)\n",
    "\n",
    "        df = df[df['rul (months)'] >= random_integer]\n",
    "        df.to_csv(directory + '/' + file_name, index=False)\n",
    "    else:\n",
    "        df = df[df['rul (months)'] > 0]\n",
    "        df.to_csv(directory + '/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = 'pseudo_testing_data_with_truth'\n",
    "directory_student = 'pseudo_testing_data'\n",
    "\n",
    "if not os.path.exists(directory_student):\n",
    "    os.makedirs(directory_student)\n",
    "\n",
    "# Iterate over the shuffled file list and rename the files\n",
    "solution = pd.DataFrame()\n",
    "for i in range(50):\n",
    "    file_name = 'item_' + str(i) + '.csv'\n",
    "    df = pd.read_csv(directory + '/' + file_name)\n",
    "    \n",
    "    true_rul = df.iloc[-1]['rul (months)']\n",
    "    solution = pd.concat([solution, pd.DataFrame([{'item_index': 'item_{}'.format(i), \n",
    "                                     'label': 1 if true_rul<=6 else 0,\n",
    "                                     'true_rul': true_rul}])])\n",
    "    \n",
    "    df = df.drop(columns=['rul (months)'])\n",
    "    df.to_csv(directory_student + '/' + file_name, index=False)\n",
    "\n",
    "solution.to_csv(directory + '/' + 'Solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this script, you will find two folders in the directory:\n",
    "- `pseudo_testing_data_with_truth`: contains the generated pseudo testing data with the true RUL. Especially, in this folder, you will find a file `Solution.csv`, which contains the ground truth. You can directly use this file to evaluate your model.\n",
    "- `pseudo_testing_data`: contains the testing data without the true RUL.\n",
    "\n",
    "Below, you will find a script that allows you evaluate your prediction on the pseudo testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    # If you want an error message to be shown to participants, you must raise the error as a ParticipantVisibleError\n",
    "    # All other errors will only be shown to the competition host. This helps prevent unintentional leakage of solution data.\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    '''\n",
    "    This metric is customized to measure the performance of remaining useful life prediction. \n",
    "    The participant is asked to predict whether the RUL of an item is less than 6 months: 1 - if RUL<=6 and 0 otherwise.\n",
    "    In the ground truth file \"Solution.csv\", there will be a column \"true_rul\" as well as a column \"label\".\n",
    "    If the predicted label matches the ground truth, a reward of 5 will be given.\n",
    "    If it does not match, then,\n",
    "    - A penalty of -10 will be given, if truth is 1 and prediction is 0;\n",
    "    - A penalty of -1/6*true_rul will be given, if truth is 0 and prediction is 1.\n",
    "\n",
    "    TODO: Add unit tests. We recommend using doctests so your tests double as usage demonstrations for competition hosts.\n",
    "    https://docs.python.org/3/library/doctest.html\n",
    "    # This example doctest works for mean absolute error:\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"item_index\"\n",
    "    >>> solution_data = {'item_index': [0, 1, 2, 3], 'label': [1, 0, 1, 0], 'true_rul': [5, 20, 1, 6]}\n",
    "    >>> submission_data = {'item_index': [0, 1, 2, 3], 'label': [1, 0, 0, 0]}\n",
    "    >>> solution = pd.DataFrame(solution_data)\n",
    "    >>> submission = pd.DataFrame(submission_data)\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name)\n",
    "    2\n",
    "    '''\n",
    "\n",
    "    # Initialize rewards and penalties\n",
    "    reward = 2\n",
    "    penalty_false_positive = -1/60\n",
    "    penalty_false_negative = -4\n",
    "\n",
    "    # Compare labels and calculate rewards/penalties\n",
    "    rewards_penalties = []\n",
    "    for _, (sol_label, sub_label, true_rul) in enumerate(zip(solution['label'], submission['label'], solution['true_rul'])):\n",
    "        if sol_label == sub_label:\n",
    "            rewards_penalties.append(reward)\n",
    "        elif sol_label == 1 and sub_label == 0:\n",
    "            rewards_penalties.append(penalty_false_negative)\n",
    "        elif sol_label == 0 and sub_label == 1:\n",
    "            rewards_penalties.append(penalty_false_positive * true_rul)\n",
    "        else:\n",
    "            rewards_penalties.append(0)  # No reward or penalty if labels don't match   \n",
    "    \n",
    "    return sum(rewards_penalties)\n",
    "\n",
    "\n",
    "row_id_column_name = \"item_index\"\n",
    "solution = pd.read_csv('pseudo_testing_data_with_truth/Solution.csv')\n",
    "\n",
    "# Put the path to your prediction result here:\n",
    "submission = \n",
    "\n",
    "print(score(solution.copy(), submission.copy(), row_id_column_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
