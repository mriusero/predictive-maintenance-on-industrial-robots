{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cahier, vous trouverez le protocole selon lequel les séquences observées dans le jeu de données de test ont été créées. Dans le dossier `training_data/degradataion_data`, vous trouverez les $50$ séquences complètes de fonctionnement jusqu'à la défaillance. Ces séquences sont générées à partir d'un modèle théorique qui contient trois modes de défaillance :\n",
    "\n",
    "- mortalité infantile\n",
    "- défaillance du tableau de contrôle\n",
    "- croissance de fissure\n",
    "\n",
    "Les trois modes de défaillance sont \"compétitifs\", c'est-à-dire que le mode de défaillance qui survient en premier sera celui qui provoque la défaillance. Il s'agit de \"fonctionnement jusqu'à la défaillance\" puisque le processus de croissance de fissure est surveillé jusqu'à ce qu'une défaillance se produise (mais la défaillance peut être due à l'un des trois modes de défaillance).\n",
    "\n",
    "Le jeu de données de test que vous avez dans le dossier `testing_data/group_0` est créé à partir des séquences complètes de fonctionnement jusqu'à la défaillance en les tronquant aléatoirement à un moment donné $t_end$, et vous devez prédire la durée de vie utile restante à partir de ce point $t_end$. La troncature est effectuée de la manière suivante :\n",
    "- Si la séquence de fonctionnement jusqu'à la défaillance est plus courte que $7$, c'est-à-dire si le temps jusqu'à la défaillance est inférieur ou égal à $6$, nous conservons la séquence telle quelle.\n",
    "- Si la séquence de fonctionnement jusqu'à la défaillance est plus longue que $7$, c'est-à-dire si le temps jusqu'à la défaillance est supérieur à $6$, elle est tronquée à un point temporel aléatoire $t_end$, généré à partir d'une distribution uniforme de [1, ttf-1].\n",
    "\n",
    "Le code suivant générera un jeu de données de test \"pseudo\" basé sur le jeu de données d'entraînement, que vous pourrez utiliser pour évaluer la performance du modèle que vous avez développé :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "group = 0\n",
    "\n",
    "directory = 'pseudo_testing_data_with_truth'\n",
    "directory_truth = 'degradation_data'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory_truth) if f.endswith('.csv')]\n",
    "\n",
    "# Iterate over the shuffled file list and rename the files\n",
    "for i, file_name in enumerate(csv_files):\n",
    "    df = pd.read_csv(directory_truth + '/' + file_name)\n",
    "    ttf = df.iloc[0]['rul (months)']\n",
    "    if ttf >= 6:\n",
    "        random_integer = random.randint(1, ttf-1)\n",
    "\n",
    "        df = df[df['rul (months)'] >= random_integer]\n",
    "        df.to_csv(directory + '/' + file_name, index=False)\n",
    "    else:\n",
    "        df = df[df['rul (months)'] > 0]\n",
    "        df.to_csv(directory + '/' + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = 'pseudo_testing_data_with_truth'\n",
    "directory_student = 'pseudo_testing_data'\n",
    "\n",
    "if not os.path.exists(directory_student):\n",
    "    os.makedirs(directory_student)\n",
    "\n",
    "# Iterate over the shuffled file list and rename the files\n",
    "solution = pd.DataFrame()\n",
    "for i in range(50):\n",
    "    file_name = 'item_' + str(i) + '.csv'\n",
    "    df = pd.read_csv(directory + '/' + file_name)\n",
    "    \n",
    "    true_rul = df.iloc[-1]['rul (months)']\n",
    "    solution = pd.concat([solution, pd.DataFrame([{'item_index': 'item_{}'.format(i), \n",
    "                                     'label': 1 if true_rul<=6 else 0,\n",
    "                                     'true_rul': true_rul}])])\n",
    "    \n",
    "    df = df.drop(columns=['rul (months)'])\n",
    "    df.to_csv(directory_student + '/' + file_name, index=False)\n",
    "\n",
    "solution.to_csv(directory + '/' + 'Solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this script, you will find two folders in the directory:\n",
    "- `pseudo_testing_data_with_truth`: contains the generated pseudo testing data with the true RUL. Especially, in this folder, you will find a file `Solution.csv`, which contains the ground truth. You can directly use this file to evaluate your model.\n",
    "- `pseudo_testing_data`: contains the testing data without the true RUL.\n",
    "\n",
    "Below, you will find a script that allows you evaluate your prediction on the pseudo testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    # If you want an error message to be shown to participants, you must raise the error as a ParticipantVisibleError\n",
    "    # All other errors will only be shown to the competition host. This helps prevent unintentional leakage of solution data.\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    '''\n",
    "    This metric is customized to measure the performance of remaining useful life prediction. \n",
    "    The participant is asked to predict whether the RUL of an item is less than 6 months: 1 - if RUL<=6 and 0 otherwise.\n",
    "    In the ground truth file \"Solution.csv\", there will be a column \"true_rul\" as well as a column \"label\".\n",
    "    If the predicted label matches the ground truth, a reward of 5 will be given.\n",
    "    If it does not match, then,\n",
    "    - A penalty of -10 will be given, if truth is 1 and prediction is 0;\n",
    "    - A penalty of -1/6*true_rul will be given, if truth is 0 and prediction is 1.\n",
    "\n",
    "    TODO: Add unit tests. We recommend using doctests so your tests double as usage demonstrations for competition hosts.\n",
    "    https://docs.python.org/3/library/doctest.html\n",
    "    # This example doctest works for mean absolute error:\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"item_index\"\n",
    "    >>> solution_data = {'item_index': [0, 1, 2, 3], 'label': [1, 0, 1, 0], 'true_rul': [5, 20, 1, 6]}\n",
    "    >>> submission_data = {'item_index': [0, 1, 2, 3], 'label': [1, 0, 0, 0]}\n",
    "    >>> solution = pd.DataFrame(solution_data)\n",
    "    >>> submission = pd.DataFrame(submission_data)\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name)\n",
    "    2\n",
    "    '''\n",
    "\n",
    "    # Initialize rewards and penalties\n",
    "    reward = 2\n",
    "    penalty_false_positive = -1/60\n",
    "    penalty_false_negative = -4\n",
    "\n",
    "    # Compare labels and calculate rewards/penalties\n",
    "    rewards_penalties = []\n",
    "    for _, (sol_label, sub_label, true_rul) in enumerate(zip(solution['label'], submission['label'], solution['true_rul'])):\n",
    "        if sol_label == sub_label:\n",
    "            rewards_penalties.append(reward)\n",
    "        elif sol_label == 1 and sub_label == 0:\n",
    "            rewards_penalties.append(penalty_false_negative)\n",
    "        elif sol_label == 0 and sub_label == 1:\n",
    "            rewards_penalties.append(penalty_false_positive * true_rul)\n",
    "        else:\n",
    "            rewards_penalties.append(0)  # No reward or penalty if labels don't match   \n",
    "    \n",
    "    return sum(rewards_penalties)\n",
    "\n",
    "\n",
    "row_id_column_name = \"item_index\"\n",
    "solution = pd.read_csv('pseudo_testing_data_with_truth/Solution.csv')\n",
    "\n",
    "# Put the path to your prediction result here:\n",
    "submission = \n",
    "\n",
    "print(score(solution.copy(), submission.copy(), row_id_column_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
